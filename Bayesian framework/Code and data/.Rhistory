#Stem words (Porters algoritm)
corpus <- tm_map(corpus, stemDocument)
## Remove punctuation from the text
corpus <- tm_map(corpus, removePunctuation)
## Remove numbers from the text
corpus <- tm_map(corpus, removeNumbers)
#Remove additional words
corpus <- tm_map(corpus, removeWords, c("the"))
writeLines(as.character(corpus[[1]])) #Check wether pre-processing worked
#Stage the data
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
nrow(dtm)
ncol(dtm)
#Explore data
findFreqTerms(dtm, lowfreq = 1000)
findAssocs(dtm, "trump", 0.4)
findAssocs(dtm, "clinton", 0.4)
findAssocs(dtm, "black", 0.6)
#Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.7)
#Plot
freq <- colSums(as.matrix(dtm))
freq <- sort(freq, decreasing = TRUE)
#Wordcloud
wordcloud(names(freq),
freq,
min.freq=400,
random.order = F,
colors =brewer.pal(6, "Set2"))
#Change data structure and add dependent variable
dtm.df <- as.matrix(dtm)
dtm.df <- as.data.frame(dtm.df)
dtm.df$fake <- data$label
dtm.df$fake <- mapvalues(dtm.df$fake, from = c("FAKE","REAL"), to=c(1,0))
dtm.df$fake <- as.numeric(dtm.df$fake)
#Dividing into training and validatio set
n <- nrow(data)
training.sample <- sample(1:n, (n*0.8))
training <- dtm.df[training.sample,]
validation <-  dtm.df[-training.sample,]
#Check dimensions
dim(training)
dim(validation)
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Create classification table
tree.predict <- as.numeric(as.vector(tree_predictions))
tree.matrix <- table(tree.predict,validation$fake)
#Fit measures
tree.accuracy <- (tree.matrix[1,1] + tree.matrix[2,2]) / sum(tree.matrix)
tree.sensitivity <- tree.matrix[1,1] / (tree.matrix[1,1] + tree.matrix[2,1])
tree.specificity <- tree.matrix[2,2] / (tree.matrix[1,2] + tree.matrix[2,2])
tree.ppv <- tree.matrix[1,1] / (tree.matrix[1,1] + tree.matrix[1,2])
tree.npv <- tree.matrix[2,2] / (tree.matrix[2,1] + tree.matrix[2,2])
tree.f1 <- (2*tree.sensitivity*tree.ppv) / (tree.sensitivity+tree.ppv)
tree.predict
tree.matrix <- table(tree.predict,validation$fake)
tree.matrix
#Fit measures
tree.accuracy <- (tree.matrix[1,1] + tree.matrix[2,2]) / sum(tree.matrix)
tree.accuracy
findAssocs(dtm, "black", 0.6)
findAssocs(dtm, "black", 0.6)
indAssocs(dtm, "black", 0.6)
findAssocs(dtm, "black", 0.6)
findAssocs(dtm, "black", 0.6)
findAssocs(dtm, "black", 0.6)
findAssocs(dtm, "clinton", 0.4)
findAssocs(dtm, "clinton", 0.4)
findAssocs(dtm, "black", 0.6)
findAssocs(dtm, "obama", 0.4)
findAssocs(dtm, "obama", 0.3)
findAssocs(dtm, "epstein", 0.3)
findAssocs(dtm, "epstein", 0.2)
findAssocs(dtm, "epstein", 0.1)
findAssocs(dtm, "clinton", 0.4)
findAssocs(dtm, "trump", 0.4)
findAssocs(dtm, "people", 0.5)
findAssocs(dtm, "peopl", 0.5)
findAssocs(dtm, "netherlands", 0.5)
findAssocs(dtm, "dutch", 0.5)
findAssocs(dtm, "russia", 0.5)
findAssocs(dtm, "russi", 0.5)
findAssocs(dtm, "russ", 0.5)
findAssocs(dtm, "soviet", 0.5)
8.3 * 0.5
6.5*0.4
(9.25 + 8.8) / 2
9.025*0.1
0.9025 + 2.6 + 4.15
knitr::opts_chunk$set(echo = TRUE)
dtm.df
knitr::opts_chunk$set(echo = TRUE)
set.seed(100)
setwd("~/Documents/Master/Psychometrics/Week 14")
library(foreign)
library(tm)
library(wordcloud)
library(rpart)
library(plyr)
library(e1071)
################################# 1. Load the data #################################
data <- read.csv("Data_2.csv", header = T, stringsAsFactors = F)
corpus <- VCorpus(VectorSource( data$text )) #Create Corpus
meta(corpus, "real") <- data$label=="REAL" # Label text
corpus <- tm_map(corpus, content_transformer(iconv), from = "latin1", to = "ASCII", sub = "" )
#Remove all words from the stop-list
corpus <- tm_map(corpus, removeWords, stopwords("en"))
#Translate upper case to lower case letters
corpus <- tm_map(corpus, content_transformer(tolower))
#Stem words (Porters algoritm)
corpus <- tm_map(corpus, stemDocument)
## Remove punctuation from the text
corpus <- tm_map(corpus, removePunctuation)
## Remove numbers from the text
corpus <- tm_map(corpus, removeNumbers)
#Remove additional words
corpus <- tm_map(corpus, removeWords, c("the"))
writeLines(as.character(corpus[[1]])) #Check wether pre-processing worked
#Stage the data
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
nrow(dtm)
ncol(dtm)
#Explore data
findFreqTerms(dtm, lowfreq = 1000)
findAssocs(dtm, "trump", 0.4)
findAssocs(dtm, "clinton", 0.4)
findAssocs(dtm, "black", 0.6)
#Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.7)
#Plot
freq <- colSums(as.matrix(dtm))
freq <- sort(freq, decreasing = TRUE)
#Wordcloud
wordcloud(names(freq),
freq,
min.freq=400,
random.order = F,
colors =brewer.pal(6, "Set2"))
#Change data structure and add dependent variable
dtm.df <- as.matrix(dtm)
dtm.df <- as.data.frame(dtm.df)
dtm.df$fake <- data$label
dtm.df$fake <- mapvalues(dtm.df$fake, from = c("FAKE","REAL"), to=c(1,0))
dtm.df$fake <- as.numeric(dtm.df$fake)
#Dividing into training and validatio set
n <- nrow(data)
training.sample <- sample(1:n, (n*0.8))
training <- dtm.df[training.sample,]
validation <-  dtm.df[-training.sample,]
#Check dimensions
dim(training)
dim(validation)
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Create classification table
tree.predict <- as.numeric(as.vector(tree_predictions))
tree.matrix <- table(tree.predict,validation$fake)
#Fit measures
tree.accuracy <- (tree.matrix[1,1] + tree.matrix[2,2]) / sum(tree.matrix)
tree.sensitivity <- tree.matrix[1,1] / (tree.matrix[1,1] + tree.matrix[2,1])
tree.specificity <- tree.matrix[2,2] / (tree.matrix[1,2] + tree.matrix[2,2])
tree.ppv <- tree.matrix[1,1] / (tree.matrix[1,1] + tree.matrix[1,2])
tree.npv <- tree.matrix[2,2] / (tree.matrix[2,1] + tree.matrix[2,2])
tree.f1 <- (2*tree.sensitivity*tree.ppv) / (tree.sensitivity+tree.ppv)
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "linear", cost = 10, scale = FALSE, type = "C")
#Predicting
svm.predict <- predict(svmfit, validation, type = "class")
#Create classification table
svm.predict <- as.numeric(as.vector(svm.predict))
svm.matrix <- table(svm.predict,validation$fake)
#Fit measures
svm.accuracy <- (svm.matrix[1,1] + svm.matrix[2,2]) / sum(svm.matrix)
svm.sensitivity <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[2,1])
svm.specificity <- svm.matrix[2,2] / (svm.matrix[1,2] + svm.matrix[2,2])
svm.ppv <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[1,2])
svm.npv <- svm.matrix[2,2] / (svm.matrix[2,1] + svm.matrix[2,2])
svm.f1 <- (2*svm.specificity*svm.ppv) / (svm.specificity+svm.ppv)
#Fitting the model
glm.fit = glm(fake ~ ., data = training, family = binomial)
#Predicting
glm.predict <- round(predict(glm.fit, validation, type="response"))
#Create classification table
glm.predict <- as.numeric(as.vector(glm.predict))
glm.matrix <- table(glm.predict,validation$fake)
#Fit measures
glm.accuracy <- (glm.matrix[1,1] + glm.matrix[2,2]) / sum(glm.matrix)
glm.sensitivity <- glm.matrix[1,1] / (glm.matrix[1,1] + glm.matrix[2,1])
glm.specificity <- glm.matrix[2,2] / (glm.matrix[1,2] + glm.matrix[2,2])
glm.ppv <- glm.matrix[1,1] / (glm.matrix[1,1] + glm.matrix[1,2])
glm.npv <- glm.matrix[2,2] / (glm.matrix[2,1] + glm.matrix[2,2])
glm.f1 <- (2*glm.specificity*glm.ppv) / (glm.specificity+glm.ppv)
dtm.df
dtm
#Stage the data
dtm <- DocumentTermMatrix(corpus)
dtm
View(dtm)
View(dtm)
dtm
view(dtm)
inspect(dtm)
nrow(dtm)
ncol(dtm)
dim(dtm)
dim(dtm)
dim(data)
#Explore data
findFreqTerms(dtm, lowfreq = 1000)
inspect(dtm)
dim(dtm)
#Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.7)
knitr::opts_chunk$set(echo = TRUE)
set.seed(100)
setwd("~/Documents/Master/Psychometrics/Week 14")
library(foreign)
library(tm)
library(wordcloud)
library(rpart)
library(plyr)
library(e1071)
### Loading the data
data <- read.csv("Data_2.csv", header = T, stringsAsFactors = F)
# Creating Corpus
corpus <- VCorpus(VectorSource( data$text )) #Create Corpus
meta(corpus, "real") <- data$label=="REAL" # Label text
corpus <- tm_map(corpus, content_transformer(iconv), from = "latin1", to = "ASCII", sub = "" )
#Remove all words from the stop-list
corpus <- tm_map(corpus, removeWords, stopwords("en"))
#Translate upper case to lower case letters
corpus <- tm_map(corpus, content_transformer(tolower))
#Stem words
corpus <- tm_map(corpus, stemDocument)
## Remove punctuation from the text
corpus <- tm_map(corpus, removePunctuation)
## Remove numbers from the text
corpus <- tm_map(corpus, removeNumbers)
#Remove additional words
corpus <- tm_map(corpus, removeWords, c("the"))
#Inspect first article
writeLines(as.character(corpus[[1]])) #Check wether pre-processing worked
#Stage the data
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
dim(dtm)
#Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.7)
#Plot
freq <- colSums(as.matrix(dtm))
freq <- sort(freq, decreasing = TRUE)
#Wordcloud
wordcloud(names(freq),
freq,
min.freq=400,
random.order = F,
colors =brewer.pal(6, "Set2"))
#Change data structure and add dependent variable
dtm.df <- as.matrix(dtm)
dtm.df <- as.data.frame(dtm.df)
dtm.df$fake <- data$label
dtm.df$fake <- mapvalues(dtm.df$fake, from = c("FAKE","REAL"), to=c(1,0))
dtm.df$fake <- as.numeric(dtm.df$fake)
#Dividing into training and validatio set
n <- nrow(data)
training.sample <- sample(1:n, (n*0.8))
training <- dtm.df[training.sample,]
validation <-  dtm.df[-training.sample,]
#Check dimensions
dim(training)
dim(validation)
#Wordcloud
wordcloud(names(freq),
freq,
min.freq=300,
random.order = F,
colors =brewer.pal(6, "Set2"))
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
text(tree, use.n = TRUE, all = F
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
```{r fig4, out.width = '200%'}
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .8)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = .3)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 1)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 4)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 2)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 1)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 1, colour = "red")
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 1)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Define tree
tree <- rpart(fake ~., data=training, method = "class")
#Plot the tree
plot(tree)
text(tree, use.n = TRUE, all = TRUE, cex = 1)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "linear", cost = 10, scale = FALSE, type = "C")
#Predicting
svm.predict <- predict(svmfit, validation, type = "class")
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "linear", cost = 10, scale = FALSE, type = "C")
#Predicting
svm.predict <- predict(svmfit, validation, type = "class")
#Create classification table
svm.predict <- as.numeric(as.vector(svm.predict))
svm.matrix <- table(svm.predict,validation$fake)
#Fit measures
svm.accuracy <- (svm.matrix[1,1] + svm.matrix[2,2]) / sum(svm.matrix)
svm.sensitivity <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[2,1])
svm.specificity <- svm.matrix[2,2] / (svm.matrix[1,2] + svm.matrix[2,2])
svm.ppv <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[1,2])
svm.npv <- svm.matrix[2,2] / (svm.matrix[2,1] + svm.matrix[2,2])
svm.f1 <- (2*svm.specificity*svm.ppv) / (svm.specificity+svm.ppv)
svm.accuracy
svm.sensitivity
svm.specificity
svm.ppv
svm.npv
svm.f1
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "polynomial", cost = 10, scale = FALSE, type = "C")
#Predicting
svm.predict <- predict(svmfit, validation, type = "class")
#Create classification table
svm.predict <- as.numeric(as.vector(svm.predict))
svm.matrix <- table(svm.predict,validation$fake)
#Fit measures
svm.accuracy <- (svm.matrix[1,1] + svm.matrix[2,2]) / sum(svm.matrix)
svm.sensitivity <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[2,1])
svm.specificity <- svm.matrix[2,2] / (svm.matrix[1,2] + svm.matrix[2,2])
svm.ppv <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[1,2])
svm.npv <- svm.matrix[2,2] / (svm.matrix[2,1] + svm.matrix[2,2])
svm.f1 <- (2*svm.specificity*svm.ppv) / (svm.specificity+svm.ppv)
svm.predict
svm.accuracy
svm.sensitivity
svm.specificity
svm.ppv
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "sigmund", cost = 10, scale = FALSE, type = "C")
#Fitting the model
svmfit = svm(fake ~ ., data = training, kernel = "sigmoid", cost = 10, scale = FALSE, type = "C")
#Predicting
svm.predict <- predict(svmfit, validation, type = "class")
#Create classification table
svm.predict <- as.numeric(as.vector(svm.predict))
svm.matrix <- table(svm.predict,validation$fake)
#Fit measures
svm.accuracy <- (svm.matrix[1,1] + svm.matrix[2,2]) / sum(svm.matrix)
svm.sensitivity <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[2,1])
svm.specificity <- svm.matrix[2,2] / (svm.matrix[1,2] + svm.matrix[2,2])
svm.ppv <- svm.matrix[1,1] / (svm.matrix[1,1] + svm.matrix[1,2])
svm.npv <- svm.matrix[2,2] / (svm.matrix[2,1] + svm.matrix[2,2])
svm.f1 <- (2*svm.specificity*svm.ppv) / (svm.specificity+svm.ppv)
svm.predict
svm.accuracy
svm.sensitivity
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(svmfit, box.palette = "RdBu", shadow.col = "gray", nn = TRUE, cex = 3)
#Predict values
tree_predictions <- predict(tree, validation, type = "class")
#Plot the tree
rpart.plot(tree, box.palette = "RdBu", shadow.col = "gray", nn = TRUE, cex = 3)
#Plot the tree
rpart.plot(tree, box.palette = "RdOr", shadow.col = "gray", nn = TRUE, cex = 3)
#Plot the tree
rpart.plot(tree, box.palette = "RdOr", shadow.col = "gray", nn = TRUE, cex = 1)
#Plot the tree
rpart.plot(tree, box.palette = "RdOr", shadow.col = "gray", nn = TRUE, cex = 2)
#Plot the tree
rpart.plot(tree, box.palette = "RdOr", shadow.col = "gray", nn = TRUE, cex = 1.8)
#Plot the tree
rpart.plot(tree, box.palette = "RdOr", shadow.col = "gray", nn = TRUE, cex = 1.5)
pbinom(0,45005,(1/5000))
0.0001231755*100
pbinom(0,45005,(1/5000),lower.tail = F)
pbinom(1,136,(1/5000))
pbinom(0,136,(1/5000))
1- 0.9731639
0.0268361*100
setwd("~/Documents/Master/Bayesian statistics/Assignment/Assignment Bart-Jan/Code and data")
#################################################################
###################### 0. PREREQUISITES #########################
#################################################################
#Loading required packages
library(tidyverse)
library(plyr)
library(gridExtra)
library(plyr)
library(MASS)
#################################################################
###################### I. SAMPLING ##############################
#################################################################
source("Gibs_no_int.R")
view(gibtropolis) #View source code Gibbs sampling WITHOUTH interaction
#################################################################
###################### VII. BAYES FACTOR ########################
#################################################################
source("Bayes_Factor.R")
view(BayesFactor)
